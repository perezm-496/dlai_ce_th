{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab34f97",
   "metadata": {},
   "source": [
    "Draft of the asignment and test the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa52b31",
   "metadata": {},
   "source": [
    "# Task #1\n",
    "\n",
    "A simple linear feedforward, or perceptron, layer consists of nodes (neurons) that transform input data using weights, biases, and an activation function. Here's a basic description of how it works:\n",
    "\n",
    "1. *Input Vector ($X$)*:\n",
    "   - The input layer consists of a vector $X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$, where each $x_i$ is a feature of the input. In this context, the vector will be a column vector; in other situations, row vectors are used.\n",
    "\n",
    "2. *Weights ($W$)*:\n",
    "   - Each input is connected to the neurons in the perceptron layer through weights. For a neuron $j$, this is represented as a vector $W_j = [w_{1j}, w_{2j}, \\ldots, w_{nj}]$. In a complete layer, all the weights are stacked in a single matrix:\n",
    "   $$W = \\begin{bmatrix} W_1 \\\\ W_2 \\\\ \\vdots \\\\ W_m \\end{bmatrix}$$\n",
    "\n",
    "3. *Bias ($b$)*:\n",
    "   - Each neuron has an associated bias term, $b_j$, which allows the activation function to be shifted. As in the case of the weights $W$, a single bias vector is used to store all the bias values for each neuron.\n",
    "\n",
    "4. *Linear Transformation*:\n",
    "   - The neuron computes a linear combination of its inputs: $z_j = W_j \\cdot X + b_j$.\n",
    "   - In matrix terms (for multiple neurons), this can be written as $Z = W \\times X + b$, where $W$ is the weight matrix, $X$ is the input vector, $b$ is the bias vector, and $\\times$ denotes matrix multiplication.\n",
    "\n",
    "5. *Activation Function ($g$)*:\n",
    "   - The linear output $z_j$ is passed through a non-linear activation function (e.g., sigmoid, ReLU) to introduce non-linearity into the model: $a_j = g(z_j)$.\n",
    "   - For the sigmoid function: $g(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "6. *Output*:\n",
    "   - The activation $a_j$, which is the output of the neuron, is passed to the next layer or becomes the final output if it's the last layer in the network.\n",
    "\n",
    "In summary, the output of the perceptron layer can be computed in vectorial form as:\n",
    "\n",
    "$$ \\mathbf{a} = g\\left( W \\, X + b \\right) $$\n",
    "\n",
    "Notice that this is the output of a single layer; you can stack several layers to produce a deeper neural network.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "Your task will be to implement forward propagation in a neural network, *as decribed in the lecture*.\n",
    "\n",
    "Let's start by computing the forward pass for a single perceptron layer using NumPy and the previously mentioned equations. Once you finish, you can test your code.\n",
    "\n",
    "Note the following:\n",
    "\n",
    "1. The input will always be a vector of shape $(N, )$, where $N$ is the number of features. Make sure to understand how NumPy manages these cases when dealing with matrices.\n",
    "\n",
    "2. The weight matrix $W$ will always be a 2D array of shape $(m, N)$, where $m$ is the number of neurons. So, in the case of a single neuron with two inputs, the matrix $W$ will have a shape of $(1, 2)$. For two neurons with 5 features, the weights matrix $W$ will have a shape of $(2, 5)$.\n",
    "\n",
    "3. In the provided code, the activation function defaults to 'sigmoid', but it can use other activation functions. Ensure this feature is implemented.\n",
    "\n",
    "**It is important to note that this description is based on a more mathematical context, where the feature vectors are supposed to be column vectors. When using other frameworks for deep learning, the feature vectors are supposed to be row vectors. The difference might be significant when working with other frameworks, so always check.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b92e42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W: (1, 3)\n",
      "Shape of bias vector: (1,)\n",
      "Shape of input: (3,)\n",
      "‚úÖ Test Case 1: 5 features, three neurons ... passed\n",
      "‚úÖ Test Case 2: 6 features, ten neurons ... passed\n",
      "‚úÖ Test Case 3: custom activation function ... passed\n",
      "‚úÖ Test Case 4: Single input, single output ... passed.\n",
      "‚úÖ Test Case 5: zero weights, zero bias ... passed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Forward pass for a single perceptron layer.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from assignment_test import test_task1\n",
    "\n",
    "# Weights and bias are given\n",
    "W = np.array([[1, 0.5, 1]]) # You will leran more about initialzing the weights later.\n",
    "b = np.array([-1.0]) # Notice this is a three input single output perceptron.\n",
    "# A sample feature vector\n",
    "X = np.array([0.9, 0.7, 0.3])\n",
    "\n",
    "# The sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Write your code here\n",
    "def perceptron_forward(x, W, b, activation=sigmoid):\n",
    "    \"\"\"\n",
    "    Computes the passforward for a feture input X\n",
    "    \"\"\"\n",
    "    # Write your solution here!!!\n",
    "    # a = np.zeros(W.shape[1])\n",
    "    a = activation(np.matmul(W, x)+b)\n",
    "    return a\n",
    "\n",
    "# Notice the shape of the weight matrix, bias and input\n",
    "print(f\"Shape of W: {W.shape}\")\n",
    "print(f\"Shape of bias vector: {b.shape}\")\n",
    "print(f\"Shape of input: {X.shape}\")\n",
    "\n",
    "\n",
    "# First a simple try\n",
    "# a = perceptron_forward(X, W, b)\n",
    "# print(f\"Activation output: {a}\\n\")\n",
    "\n",
    "test_task1(perceptron_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0f088ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå First Test Failed\n",
      "Sugestion:\n",
      "üîß Seems like you might have forgoten to apply the activation function.\n",
      "\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚ùå Test 2 Failed\n",
      "\n",
      "The forward result is not the expected, when creating 10 neurons for six inputs the value was not the expected.   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚úÖ Test Case 3: custom activation function ... passed\n",
      "‚ùå Test 4 Failed\n",
      "\n",
      "The forward result is not the expected,when using a single input (X.shape=(1,)), and a single neuron (W.shape=(1,1)).you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "   - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚ùå Test 5 Failed\n",
      "\n",
      "The forward result is not the expected,when using weights and bias zero. You might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "   - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def foo1(x, W, b, activation=sigmoid):\n",
    "    return np.matmul(W, x) + b\n",
    "\n",
    "test_task1(foo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9700ff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå First Test Failed\n",
      "Sugestion:\n",
      "üîß Did you add the bias term?\n",
      "\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚ùå Test 2 Failed\n",
      "\n",
      "The forward result is not the expected, when creating 10 neurons for six inputs the value was not the expected.   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚ùå Test 3 Failed\n",
      "\n",
      "The forward result is not the expected,when using an activation function diferent to the default value.You might want to check the following:\n",
      "   - Are you using the parameter sent to the function 'activation' and not the default value 'sigmoid'.\n",
      "‚ùå Test 4 Failed\n",
      "\n",
      "The forward result is not the expected,when using a single input (X.shape=(1,)), and a single neuron (W.shape=(1,1)).you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "   - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚úÖ Test Case 5: zero weights, zero bias ... passed\n"
     ]
    }
   ],
   "source": [
    "def foo2(x, W, b, activation=sigmoid):\n",
    "    return activation(np.matmul(W, x))\n",
    "\n",
    "test_task1(foo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61be6a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå First Test Failed\n",
      "Sugestion:\n",
      "üîß  Are you sure to be using the correct product order W√óX?\n",
      "\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚ùå Test 2 Failed\n",
      "\n",
      "The forward result is not the expected, when creating 10 neurons for six inputs the value was not the expected.   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚ùå Test 3 Failed\n",
      "\n",
      "The forward result is not the expected,when using an activation function diferent to the default value.You might want to check the following:\n",
      "   - Are you using the parameter sent to the function 'activation' and not the default value 'sigmoid'.\n",
      "‚úÖ Test Case 4: Single input, single output ... passed.\n",
      "‚ùå Test 5 Failed\n",
      "\n",
      "The forward result is not the expected,when using weights and bias zero. You might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "   - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def foo3(x, W, b, activation=sigmoid):\n",
    "    return activation(np.matmul(x, W) + b)\n",
    "\n",
    "test_task1(foo3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3492f852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9525741268224334\n",
      "‚ùå First Test Failed\n",
      "Sugestion:\n",
      "\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚ùå Test 2 Failed\n",
      "\n",
      "The forward result is not the expected, when creating 10 neurons for six inputs the value was not the expected.   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚ùå Test 3 Failed\n",
      "\n",
      "The forward result is not the expected,when using an activation function diferent to the default value.You might want to check the following:\n",
      "   - Are you using the parameter sent to the function 'activation' and not the default value 'sigmoid'.\n",
      "‚ùå Test 4 Failed\n",
      "\n",
      "The forward result is not the expected,when using a single input (X.shape=(1,)), and a single neuron (W.shape=(1,1)).you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "   - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n",
      "‚ùå Test 5 Failed\n",
      "\n",
      "The forward result is not the expected,when using weights and bias zero. You might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "   - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def foo4(x, W, b, activation=sigmoid):\n",
    "    return activation(np.sum([wj*xj for wj, xj in zip(x, W)]))\n",
    "\n",
    "print(foo4(np.ones(3), np.ones(3), np.ones(3)))\n",
    "\n",
    "test_task1(foo4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f4a27da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test Case 1: 5 features, three neurons ... passed\n",
      "‚úÖ Test Case 2: 6 features, ten neurons ... passed\n",
      "‚ùå Test 3 Failed\n",
      "\n",
      "The forward result is not the expected,when using an activation function diferent to the default value.You might want to check the following:\n",
      "   - Are you using the parameter sent to the function 'activation' and not the default value 'sigmoid'.\n",
      "‚úÖ Test Case 4: Single input, single output ... passed.\n",
      "‚úÖ Test Case 5: zero weights, zero bias ... passed\n"
     ]
    }
   ],
   "source": [
    "def foo5(x, W, b, activation=sigmoid):\n",
    "    return sigmoid(np.matmul(W, x) + b)\n",
    "\n",
    "test_task1(foo5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c48d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
