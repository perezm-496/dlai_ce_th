{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab34f97",
   "metadata": {},
   "source": [
    "Draft of the asignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa52b31",
   "metadata": {},
   "source": [
    "# Task #1\n",
    "\n",
    "A feedforward simple linear or perceptron layer consists of nodes (neurons) that transform input data using weights, biases, and an activation function. Here's a basic description of how it works: \n",
    "\n",
    "1. *Input Vector ($X$)*:\n",
    "   - The input layer consists of a vector $X = [x_1, x_2, \\ldots, x_n]$, where each $x_i$ is a feature of the input.\n",
    "\n",
    "2. *Weights ($W$)*:\n",
    "   - Each input is connected to the neurons in the perceptron layer through weights. For a neuron $j$, this is represented as a vector $W_j = [w_{1j}, w_{2j}, \\ldots, w_{nj}]$.\n",
    "\n",
    "3. *Bias ($b$)*:\n",
    "   - Each neuron has an associated bias term, $b_j$, which allows the activation function to be shifted.\n",
    "\n",
    "4. *Linear Transformation*:\n",
    "   - The neuron computes a linear combination of its inputs: $z_j = W_j \\cdot X + b_j$.\n",
    "   - In matrix terms (for multiple neurons), this can be written as $Z = WX + b$, where $W$ is the weight matrix, $X$ is the input vector, and $b$ is the bias vector.\n",
    "\n",
    "5. *Activation Function ($g$)*:\n",
    "   - The linear output $z_j$ is passed through a non-linear activation function (e.g., sigmoid, ReLU) to introduce non-linearity into the model: $a_j = g(z_j)$.\n",
    "   - For the sigmoid function: $g(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "6. /Output/:\n",
    "   - The activation $a_j$, which is the output of the neuron, is passed to the next layer or becomes the final output if it's the last layer in the network.\n",
    "\n",
    "So in summary the output of the perceptron layer can be computed in vectorial form as:\n",
    "\n",
    "$$ \\mathbf{a_1} = g\\left( W_1 \\, X + b_1 \\right)$$\n",
    "\n",
    "Notice that this is the output of a single layer, you can stack several layers to produce a deeper neural network.\n",
    "\n",
    "# Instructions \n",
    "Your task will be to implement the forward propagation on a neural network.\n",
    "\n",
    "Let's start by computing the pass forward for a single perceptron layer using NumPy and the previous equations. Once you finish you can test your code.\n",
    "\n",
    "**It is important to notice that this description is based on a more mathematical description where the feature vectors are supposed to be column vectors. When using other frameworks for deep learning the feature vectors are supposed to be row vectors. The difference might be significative when you start working with other frameworks so always check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b92e42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation output: [0.549834   0.24973989 0.69444857]\n",
      "✅ Test: Activation function applied ... passed \n",
      "✅ Test: Bias added ... passed \n",
      "✅ Test: Basic pass forward ... passed \n",
      "✅ Test: Three inputs single output test ... passed \n",
      "correctness_2_3 test passed\n",
      "zero_weights test passed\n",
      "zero_inputs test passed\n",
      "different_activation test passed\n",
      "bias_well_managed test passed\n",
      "weights_correctly_applied test passed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Forward pass for a single perceptron layer.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from assignment_test import test_task1\n",
    "\n",
    "# Weights and bias are given\n",
    "W = np.array([\n",
    "    [1, 0, 1],\n",
    "    [-1, 0, -1],\n",
    "    [0.1, 1, 0.1]]) # You will leran more about initialzing the weights later.\n",
    "b = np.array([-1.0, 0.1, 0.001])\n",
    "# A sample feature vector\n",
    "X = np.array([0.9, 0.7, 0.3])\n",
    "\n",
    "# The sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Write your code here\n",
    "def perceptron_forward(x, W, b, activation=sigmoid):\n",
    "    \"\"\"\n",
    "    Computes the passforward for a feture input X\n",
    "    \"\"\"\n",
    "    # Write your solution here!!!\n",
    "    a = np.zeros(W.shape[1])\n",
    "    a = activation(np.dot(W, x) + b)\n",
    "    return a\n",
    "\n",
    "# First a simple try\n",
    "a = perceptron_forward(X, W, b)\n",
    "print(f\"Activation output: {a}\")\n",
    "\n",
    "# Now let's test it\n",
    "test_task1(perceptron_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f088ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test: Activation function applied ... passed \n",
      "✅ Test: Bias added ... passed \n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n",
      "\n",
      "[❌Three inputs single output test FAILED]\n",
      "The forward result is not the expected.\n",
      "   - Check the order of matrix multiplication.\n",
      "   - Recall that although np.matmul and np.dot are similar have small similarities\n",
      "     specially when using 1D arrays.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mmatmul(x,W)\u001b[38;5;241m+\u001b[39mb)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbad_product\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:190\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    188\u001b[0m correctness_sample()\n\u001b[1;32m    189\u001b[0m correctness_3_1()\n\u001b[0;32m--> 190\u001b[0m \u001b[43mcorrectness_2_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m zero_weights()\n\u001b[1;32m    192\u001b[0m zero_inputs()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:121\u001b[0m, in \u001b[0;36mtest_task1.<locals>.correctness_2_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m])\n\u001b[1;32m    120\u001b[0m expected_output \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(W, X) \u001b[38;5;241m+\u001b[39m b)\n\u001b[0;32m--> 121\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mperceptron_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(output, expected_output), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed correctness_2_3 test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness_2_3 test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mbad_product\u001b[0;34m(x, W, b, activation)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbad_product\u001b[39m(x, W, b, activation\u001b[38;5;241m=\u001b[39msigmoid):\n\u001b[0;32m----> 2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39mb)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)"
     ]
    }
   ],
   "source": [
    "def bad_product(x, W, b, activation=sigmoid):\n",
    "    a = sigmoid(np.matmul(x,W)+b)\n",
    "    return a\n",
    "\n",
    "test_task1(bad_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9700ff70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forgot_activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m activation(np\u001b[38;5;241m.\u001b[39mmatmul(x, W) \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m test_task1(\u001b[43mforgot_activation\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'forgot_activation' is not defined"
     ]
    }
   ],
   "source": [
    "def bad_mult(x, W, b, activation=sigmoid):\n",
    "    a = activation(np.matmul(x, W) + b)\n",
    "    return a\n",
    "\n",
    "test_task1(forgot_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efe7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[❌Activation applied sample test FAILED]\n",
      "The forward result is not the expected,\n",
      "it seeems like you fail to apply the activation function.\n",
      "\n",
      "\n",
      "[❌Bias added sample test FAILED]\n",
      "The forward result is not the expected,\n",
      "it seeems like you might not have added the bias.\n",
      "\n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n",
      "(1,)\n",
      "✅ Test: Three inputs single output test ... passed \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Failed correctness_2_3 test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(W, x) \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforgot_activation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:187\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    185\u001b[0m correctness_3_1()\n\u001b[1;32m    186\u001b[0m correctness_2_3()\n\u001b[0;32m--> 187\u001b[0m zero_weights()\n\u001b[1;32m    188\u001b[0m zero_inputs()\n\u001b[1;32m    189\u001b[0m different_activation()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:119\u001b[0m, in \u001b[0;36mcorrectness_2_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m output \u001b[38;5;241m=\u001b[39m perceptron_forward(X, W, b)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(output, expected_output), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed correctness_2_3 test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness_2_3 test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Failed correctness_2_3 test"
     ]
    }
   ],
   "source": [
    "def forgot_activation(x, W, b, activation=sigmoid):\n",
    "    a = np.matmul(W, x) + b\n",
    "    return a\n",
    "\n",
    "test_task1(forgot_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea90515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test: Activation function applied ... passed \n",
      "\n",
      "[❌Bias added sample test FAILED]\n",
      "The forward result is not the expected,\n",
      "it seeems like you might not have added the bias.\n",
      "\n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n",
      "✅ Test: Three inputs single output test ... passed \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Failed correctness_2_3 test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(W, x))\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforgot_bias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:186\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    184\u001b[0m correctness_sample()\n\u001b[1;32m    185\u001b[0m correctness_3_1()\n\u001b[0;32m--> 186\u001b[0m \u001b[43mcorrectness_2_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m zero_weights()\n\u001b[1;32m    188\u001b[0m zero_inputs()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:118\u001b[0m, in \u001b[0;36mtest_task1.<locals>.correctness_2_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m expected_output \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(W, X) \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m    117\u001b[0m output \u001b[38;5;241m=\u001b[39m perceptron_forward(X, W, b)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(output, expected_output), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed correctness_2_3 test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness_2_3 test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Failed correctness_2_3 test"
     ]
    }
   ],
   "source": [
    "def forgot_bias(x, W, b, activation=sigmoid):\n",
    "    a = sigmoid(np.dot(W, x))\n",
    "    return a\n",
    "\n",
    "test_task1(forgot_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc31113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test: Activation function applied ... passed \n",
      "\n",
      "[❌Bias added sample test FAILED]\n",
      "The forward result is not the expected,\n",
      "it seeems like you might not have added the bias.\n",
      "\n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n",
      "(1,)\n",
      "✅ Test: Three inputs single output test ... passed \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Failed correctness_2_3 test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(W, x))\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforgot_bias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:187\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    185\u001b[0m correctness_3_1()\n\u001b[1;32m    186\u001b[0m correctness_2_3()\n\u001b[0;32m--> 187\u001b[0m zero_weights()\n\u001b[1;32m    188\u001b[0m zero_inputs()\n\u001b[1;32m    189\u001b[0m different_activation()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:119\u001b[0m, in \u001b[0;36mcorrectness_2_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m output \u001b[38;5;241m=\u001b[39m perceptron_forward(X, W, b)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(output, expected_output), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed correctness_2_3 test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness_2_3 test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Failed correctness_2_3 test"
     ]
    }
   ],
   "source": [
    "def forgot_bias(x, W, b, activation=sigmoid):\n",
    "    a = sigmoid(np.dot(W, x))\n",
    "    return a\n",
    "\n",
    "test_task1(forgot_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94329196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test: Activation function applied ... passed \n",
      "✅ Test: Bias added ... passed \n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mmatmul(x, W)\u001b[38;5;241m+\u001b[39mb)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbad_product\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:189\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    187\u001b[0m bias_added()\n\u001b[1;32m    188\u001b[0m correctness_sample()\n\u001b[0;32m--> 189\u001b[0m \u001b[43mcorrectness_3_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m correctness_2_3()\n\u001b[1;32m    191\u001b[0m zero_weights()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:106\u001b[0m, in \u001b[0;36mtest_task1.<locals>.correctness_3_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m fail_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[❌Three inputs single output test FAILED]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe forward result is not the expected.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     specially when using 1D arrays.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m )\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m==\u001b[39m expected_output\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Test: Three inputs single output test ... passed \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be6a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
