{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab34f97",
   "metadata": {},
   "source": [
    "Draft of the asignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa52b31",
   "metadata": {},
   "source": [
    "# Task #1\n",
    "\n",
    "A feedforward simple linear or perceptron layer consists of nodes (neurons) that transform input data using weights, biases, and an activation function. Here's a basic description of how it works: \n",
    "\n",
    "1. *Input Vector ($X$)*:\n",
    "   - The input layer consists of a vector $X = [x_1, x_2, \\ldots, x_n]$, where each $x_i$ is a feature of the input.\n",
    "\n",
    "2. *Weights ($W$)*:\n",
    "   - Each input is connected to the neurons in the perceptron layer through weights. For a neuron $j$, this is represented as a vector $W_j = [w_{1j}, w_{2j}, \\ldots, w_{nj}]$.\n",
    "\n",
    "3. *Bias ($b$)*:\n",
    "   - Each neuron has an associated bias term, $b_j$, which allows the activation function to be shifted.\n",
    "\n",
    "4. *Linear Transformation*:\n",
    "   - The neuron computes a linear combination of its inputs: $z_j = W_j \\cdot X + b_j$.\n",
    "   - In matrix terms (for multiple neurons), this can be written as $Z = WX + b$, where $W$ is the weight matrix, $X$ is the input vector, and $b$ is the bias vector.\n",
    "\n",
    "5. *Activation Function ($g$)*:\n",
    "   - The linear output $z_j$ is passed through a non-linear activation function (e.g., sigmoid, ReLU) to introduce non-linearity into the model: $a_j = g(z_j)$.\n",
    "   - For the sigmoid function: $g(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "6. /Output/:\n",
    "   - The activation $a_j$, which is the output of the neuron, is passed to the next layer or becomes the final output if it's the last layer in the network.\n",
    "\n",
    "So in summary the output of the perceptron layer can be computed in vectorial form as:\n",
    "\n",
    "$$ \\mathbf{a_1} = g\\left( W_1 \\, X + b_1 \\right)$$\n",
    "\n",
    "Notice that this is the output of a single layer, you can stack several layers to produce a deeper neural network.\n",
    "\n",
    "# Instructions \n",
    "Your task will be to implement the forward propagation on a neural network.\n",
    "\n",
    "Let's start by computing the pass forward for a single perceptron layer using NumPy and the previous equations. Once you finish you can test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Forward pass for a single perceptron layer.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import test\n",
    "\n",
    "# Weights and bias are given\n",
    "W = np.array([\n",
    "    [1, 0, 1],\n",
    "    [-1, 0, -1],\n",
    "    [0.1, 1, 0.1]]) # You will leran more about initialzing the weights later.\n",
    "b = np.array([-1.0, 0.1, 0.001])\n",
    "# A sample feature vector\n",
    "X = np.array([0.9, 0.7, 0.3])\n",
    "\n",
    "# The sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Write your code here\n",
    "def perceptron_forward(x, W, b, activation=sigmoid):\n",
    "    \"\"\"\n",
    "    Computes the passforward for a feture input X\n",
    "    \"\"\"\n",
    "    # Write your solution here!!!\n",
    "    a = np.zeros(W.shape[1])\n",
    "    return a\n",
    "\n",
    "# First a simple try\n",
    "a = perceptron_forward(X, W, b)\n",
    "print(f\"Activation output: {a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a897d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
