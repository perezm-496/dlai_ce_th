{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab34f97",
   "metadata": {},
   "source": [
    "Draft of the asignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa52b31",
   "metadata": {},
   "source": [
    "# Task #1\n",
    "\n",
    "A simple linear feedforward, or perceptron, layer consists of nodes (neurons) that transform input data using weights, biases, and an activation function. Here's a basic description of how it works:\n",
    "\n",
    "1. *Input Vector ($X$)*:\n",
    "   - The input layer consists of a vector $X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$, where each $x_i$ is a feature of the input. In this context, the vector will be a column vector; in other situations, row vectors are used.\n",
    "\n",
    "2. *Weights ($W$)*:\n",
    "   - Each input is connected to the neurons in the perceptron layer through weights. For a neuron $j$, this is represented as a vector $W_j = [w_{1j}, w_{2j}, \\ldots, w_{nj}]$. In a complete layer, all the weights are stacked in a single matrix:\n",
    "   $$W = \\begin{bmatrix} W_1 \\\\ W_2 \\\\ \\vdots \\\\ W_m \\end{bmatrix}$$\n",
    "\n",
    "3. *Bias ($b$)*:\n",
    "   - Each neuron has an associated bias term, $b_j$, which allows the activation function to be shifted. As in the case of the weights $W$, a single bias vector is used to store all the bias values for each neuron.\n",
    "\n",
    "4. *Linear Transformation*:\n",
    "   - The neuron computes a linear combination of its inputs: $z_j = W_j \\cdot X + b_j$.\n",
    "   - In matrix terms (for multiple neurons), this can be written as $Z = W \\times X + b$, where $W$ is the weight matrix, $X$ is the input vector, $b$ is the bias vector, and $\\times$ denotes matrix multiplication.\n",
    "\n",
    "5. *Activation Function ($g$)*:\n",
    "   - The linear output $z_j$ is passed through a non-linear activation function (e.g., sigmoid, ReLU) to introduce non-linearity into the model: $a_j = g(z_j)$.\n",
    "   - For the sigmoid function: $g(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "6. *Output*:\n",
    "   - The activation $a_j$, which is the output of the neuron, is passed to the next layer or becomes the final output if it's the last layer in the network.\n",
    "\n",
    "In summary, the output of the perceptron layer can be computed in vectorial form as:\n",
    "\n",
    "$$ \\mathbf{a} = g\\left( W \\, X + b \\right) $$\n",
    "\n",
    "Notice that this is the output of a single layer; you can stack several layers to produce a deeper neural network.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "Your task will be to implement forward propagation in a neural network, *as decribed in the lecture*.\n",
    "\n",
    "Let's start by computing the forward pass for a single perceptron layer using NumPy and the previously mentioned equations. Once you finish, you can test your code.\n",
    "\n",
    "Note the following:\n",
    "\n",
    "1. The input will always be a vector of shape $(N, )$, where $N$ is the number of features. Make sure to understand how NumPy manages these cases when dealing with matrices.\n",
    "\n",
    "2. The weight matrix $W$ will always be a 2D array of shape $(m, N)$, where $m$ is the number of neurons. So, in the case of a single neuron with two inputs, the matrix $W$ will have a shape of $(1, 2)$. For two neurons with 5 features, the weights matrix $W$ will have a shape of $(2, 5)$.\n",
    "\n",
    "3. In the provided code, the activation function defaults to 'sigmoid', but it can use other activation functions. Ensure this feature is implemented.\n",
    "\n",
    "**It is important to note that this description is based on a more mathematical context, where the feature vectors are supposed to be column vectors. When using other frameworks for deep learning, the feature vectors are supposed to be row vectors. The difference might be significant when working with other frameworks, so always check.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b92e42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W: (1, 3)\n",
      "Shape of bias vector: (1,)\n",
      "Shape of input: (3,)\n",
      "✅ Test Case 1: 5 features, three neurons ... passed\n",
      "✅ Test Case 2: 6 features, ten neurons ... passed\n",
      "✅ Test Case 3: custom activation function ... passed\n",
      "✅ Test Case 3: custom activation function ... passed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Forward pass for a single perceptron layer.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from assignment_test import test_task1\n",
    "\n",
    "# Weights and bias are given\n",
    "W = np.array([[1, 0.5, 1]]) # You will leran more about initialzing the weights later.\n",
    "b = np.array([-1.0]) # Notice this is a three input single output perceptron.\n",
    "# A sample feature vector\n",
    "X = np.array([0.9, 0.7, 0.3])\n",
    "\n",
    "# The sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Write your code here\n",
    "def perceptron_forward(x, W, b, activation=sigmoid):\n",
    "    \"\"\"\n",
    "    Computes the passforward for a feture input X\n",
    "    \"\"\"\n",
    "    # Write your solution here!!!\n",
    "    # a = np.zeros(W.shape[1])\n",
    "    a = activation(np.matmul(W, x)+b)\n",
    "    return a\n",
    "\n",
    "# Notice the shape of the weight matrix, bias and input\n",
    "print(f\"Shape of W: {W.shape}\")\n",
    "print(f\"Shape of bias vector: {b.shape}\")\n",
    "print(f\"Shape of input: {X.shape}\")\n",
    "\n",
    "\n",
    "# First a simple try\n",
    "# a = perceptron_forward(X, W, b)\n",
    "# print(f\"Activation output: {a}\\n\")\n",
    "\n",
    "test_task1(perceptron_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47215449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f088ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test: Activation function applied ... passed \n",
      "✅ Test: Bias added ... passed \n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n",
      "\n",
      "[❌Three inputs single output test FAILED]\n",
      "The forward result is not the expected.\n",
      "   - Check the order of matrix multiplication.\n",
      "   - Recall that although np.matmul and np.dot are similar have small similarities\n",
      "     specially when using 1D arrays.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mmatmul(x,W)\u001b[38;5;241m+\u001b[39mb)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbad_product\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:190\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    188\u001b[0m correctness_sample()\n\u001b[1;32m    189\u001b[0m correctness_3_1()\n\u001b[0;32m--> 190\u001b[0m \u001b[43mcorrectness_2_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m zero_weights()\n\u001b[1;32m    192\u001b[0m zero_inputs()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:121\u001b[0m, in \u001b[0;36mtest_task1.<locals>.correctness_2_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m])\n\u001b[1;32m    120\u001b[0m expected_output \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(W, X) \u001b[38;5;241m+\u001b[39m b)\n\u001b[0;32m--> 121\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mperceptron_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(output, expected_output), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed correctness_2_3 test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness_2_3 test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mbad_product\u001b[0;34m(x, W, b, activation)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbad_product\u001b[39m(x, W, b, activation\u001b[38;5;241m=\u001b[39msigmoid):\n\u001b[0;32m----> 2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39mb)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)"
     ]
    }
   ],
   "source": [
    "def bad_product(x, W, b, activation=sigmoid):\n",
    "    a = sigmoid(np.matmul(x,W)+b)\n",
    "    return a\n",
    "\n",
    "test_task1(bad_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9700ff70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forgot_activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m activation(np\u001b[38;5;241m.\u001b[39mmatmul(x, W) \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m test_task1(\u001b[43mforgot_activation\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'forgot_activation' is not defined"
     ]
    }
   ],
   "source": [
    "def bad_mult(x, W, b, activation=sigmoid):\n",
    "    a = activation(np.matmul(x, W) + b)\n",
    "    return a\n",
    "\n",
    "test_task1(forgot_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efe7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[❌Activation applied sample test FAILED]\n",
      "The forward result is not the expected,\n",
      "it seeems like you fail to apply the activation function.\n",
      "\n",
      "\n",
      "[❌Bias added sample test FAILED]\n",
      "The forward result is not the expected,\n",
      "it seeems like you might not have added the bias.\n",
      "\n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n",
      "(1,)\n",
      "✅ Test: Three inputs single output test ... passed \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Failed correctness_2_3 test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(W, x) \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforgot_activation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:187\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    185\u001b[0m correctness_3_1()\n\u001b[1;32m    186\u001b[0m correctness_2_3()\n\u001b[0;32m--> 187\u001b[0m zero_weights()\n\u001b[1;32m    188\u001b[0m zero_inputs()\n\u001b[1;32m    189\u001b[0m different_activation()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:119\u001b[0m, in \u001b[0;36mcorrectness_2_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m output \u001b[38;5;241m=\u001b[39m perceptron_forward(X, W, b)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(output, expected_output), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed correctness_2_3 test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness_2_3 test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Failed correctness_2_3 test"
     ]
    }
   ],
   "source": [
    "def forgot_activation(x, W, b, activation=sigmoid):\n",
    "    a = np.matmul(W, x) + b\n",
    "    return a\n",
    "\n",
    "test_task1(forgot_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea90515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test: Activation function applied ... passed \n",
      "\n",
      "[❌Bias added sample test FAILED]\n",
      "The forward result is not the expected,\n",
      "it seeems like you might not have added the bias.\n",
      "\n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n",
      "✅ Test: Three inputs single output test ... passed \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Failed correctness_2_3 test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(W, x))\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforgot_bias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:186\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    184\u001b[0m correctness_sample()\n\u001b[1;32m    185\u001b[0m correctness_3_1()\n\u001b[0;32m--> 186\u001b[0m \u001b[43mcorrectness_2_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m zero_weights()\n\u001b[1;32m    188\u001b[0m zero_inputs()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:118\u001b[0m, in \u001b[0;36mtest_task1.<locals>.correctness_2_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m expected_output \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(W, X) \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m    117\u001b[0m output \u001b[38;5;241m=\u001b[39m perceptron_forward(X, W, b)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(output, expected_output), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed correctness_2_3 test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness_2_3 test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Failed correctness_2_3 test"
     ]
    }
   ],
   "source": [
    "def forgot_bias(x, W, b, activation=sigmoid):\n",
    "    a = sigmoid(np.dot(W, x))\n",
    "    return a\n",
    "\n",
    "test_task1(forgot_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc31113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test: Activation function applied ... passed \n",
      "\n",
      "[❌Bias added sample test FAILED]\n",
      "The forward result is not the expected,\n",
      "it seeems like you might not have added the bias.\n",
      "\n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n",
      "(1,)\n",
      "✅ Test: Three inputs single output test ... passed \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Failed correctness_2_3 test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mdot(W, x))\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforgot_bias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:187\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    185\u001b[0m correctness_3_1()\n\u001b[1;32m    186\u001b[0m correctness_2_3()\n\u001b[0;32m--> 187\u001b[0m zero_weights()\n\u001b[1;32m    188\u001b[0m zero_inputs()\n\u001b[1;32m    189\u001b[0m different_activation()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:119\u001b[0m, in \u001b[0;36mcorrectness_2_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m output \u001b[38;5;241m=\u001b[39m perceptron_forward(X, W, b)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(output, expected_output), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed correctness_2_3 test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrectness_2_3 test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Failed correctness_2_3 test"
     ]
    }
   ],
   "source": [
    "def forgot_bias(x, W, b, activation=sigmoid):\n",
    "    a = sigmoid(np.dot(W, x))\n",
    "    return a\n",
    "\n",
    "test_task1(forgot_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94329196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test: Activation function applied ... passed \n",
      "✅ Test: Bias added ... passed \n",
      "\n",
      "[❌Correctness Sample test FAILED]\n",
      "The forward result is not the expected, you might want to check the following:\n",
      "   - The order of matrix multiplication.\n",
      "   - Is the bias term present?\n",
      "   - The activation function is correctly applied.\n",
      "    - Did you use the np.dot to compute the matrix product?\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     a \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mmatmul(x, W)\u001b[38;5;241m+\u001b[39mb)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_task1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbad_product\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:189\u001b[0m, in \u001b[0;36mtest_task1\u001b[0;34m(perceptron_forward)\u001b[0m\n\u001b[1;32m    187\u001b[0m bias_added()\n\u001b[1;32m    188\u001b[0m correctness_sample()\n\u001b[0;32m--> 189\u001b[0m \u001b[43mcorrectness_3_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m correctness_2_3()\n\u001b[1;32m    191\u001b[0m zero_weights()\n",
      "File \u001b[0;32m~/Dropbox/inbox/dlai_eval/dlai_ce_th/assignment_test.py:106\u001b[0m, in \u001b[0;36mtest_task1.<locals>.correctness_3_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m fail_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[❌Three inputs single output test FAILED]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe forward result is not the expected.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     specially when using 1D arrays.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m )\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m==\u001b[39m expected_output\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Test: Three inputs single output test ... passed \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be6a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
