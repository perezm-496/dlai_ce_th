Note:

I use mostly gpt-tel in emacs with the openai API.

Interaction Log:

User Request:
> "Check the text and improve it."
Text Provided:
> "To test your implementation you just need to execute the following cell.
> The code will just sent the function you implemented to a library to run some test cases and if something goes wrong it will prompt you with some feedback."

Response:
> Improved Text:
> "To test your implementation, simply execute the following cell.
> The code will send the function you implemented to a library that will run several test cases.
> If something goes wrong, it will provide feedback to help you identify and fix the issue."

Notes:
- Corrected grammar ("sent" -> "send").
- Improved flow for clarity.
- Added slight formatting for readability.

End of Log


Check the code of this test function:
"""
"""
assignment_test.py - Test module, not really unitest although it makes unitest.
My decition to change it so I can give better feedback without the traceback.

This module contains unit tests designed to verify the correctness of student 
implementations for the exercise ce_th_assignment.ipynb. The tests will:

1. Check the accuracy and functionality of the student's code.
2. Provide clear and informative feedback for any incorrect solutions, helping 
   learners identify and correct their mistakes.

Students can execute this test module within their notebooks to independently 
assess the correctness of their code.

Usage:
- Ensure this file is in the same directory as the student's implementation.
- Run the tests to see immediate results and feedback as described in the notebook.
"""
import numpy as np
import traceback
from solution import my_perceptron_forward

def test_task1(perceptron_forward):
    # To prevent a changed simoid to produce bad results
    def sigmoid(z):
        return 1/(1+np.exp(-z))
    
    # Custom comparation to avoid exception when non array or mismatch shape is used.
    # arr1: answ
    # arr2: expectd with shapep
    def equality_check(arr1, arr2, shape=(3,)):
        if type(arr1) != np.ndarray:
            return False
        if arr1.shape != shape:
            return False
        return np.allclose(arr1, arr2)


    # Test 1: Check simple pass forward 5 features, 3 neurons
    def test1():
        np.random.seed = 2025 # reproduce
        X = np.random.rand( 5)
        W = np.random.rand( 3, 5)
        b = np.random.rand(3)
        expected = my_perceptron_forward(X, W, b)
        try:
            answ = perceptron_forward(X, W, b)
        except Exception:
            answ = np.zeros(3)
        if equality_check(answ, expected, expected.shape):
            print("‚úÖ Test Case 1: 5 features, three neurons ... passed")
        else:
            print("‚ùå First Test Failed")
            sub_test_list = [test1_1, test1_2, test1_3]
            print("Sugestion:")
            for sub_test in sub_test_list:
                sugestion = sub_test()
                if sugestion is not None:
                    print(sugestion)
                    break
            msg = (
                "\nThe forward result is not the expected, you might want to check the following:\n"
                "   - The order of matrix multiplication.\n"
                "   - Is the bias term present?\n"
                "   - The activation function is correctly applied.\n"
                "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n"
            )
            print(msg)

    # SubTest 3: Wrong order or product
    def test1_3():
        np.random.seed = 2025 # reproduce
        X = np.random.rand(3)
        W = np.random.rand(3, 3)
        b = np.random.rand(3)
        expected_error = sigmoid(np.matmul(X,W) + b)  # bad product
        try:
            answ = perceptron_forward(X, W, b)
        except Exception:
            answ = np.zeros(3)
        if equality_check(answ, expected_error, shape=expected_error.shape):
            return "üîß  Are you sure to be using the correct product order W√óX?"
        else:
            return None

    # SubTest 2: Forgot bias
    def test1_2():
        np.random.seed = 2025 # reproduce
        X = np.random.rand( 5)
        W = np.random.rand( 3, 5)
        b = np.random.rand(3)
        expected_error = sigmoid(np.matmul(W, X))  # missing bias
        try:
            answ = perceptron_forward(X, W, b)
        except Exception:
            answ = np.zeros(3)
        if equality_check(answ, expected_error, shape=expected_error.shape):
            return "üîß Did you add the bias term?"
        else:
            return None
    
    # SubTest 1: Missing Activation.
    def test1_1():
        np.random.seed = 2025 # reproduce
        X = np.random.rand( 5)
        W = np.random.rand( 3, 5)
        b = np.random.rand(3)
        expected_error = np.matmul(W, X) + b # missing activation
        try:
            answ = perceptron_forward(X, W, b)
        except Exception:
            answ = np.zeros(3)
        if equality_check(answ, expected_error, expected_error.shape):
            return "üîß Seems like you might have forgoten to apply the activation function."
        else:
            return None

    # Test 2: Checking more outputs than features
    def test2():
        np.random.seed = 2002 # reproduce
        X = np.random.rand(6)
        W = np.random.rand( 10, 6)
        b = np.random.rand(10)
        expected = my_perceptron_forward(X, W, b)
        try:
            answ = perceptron_forward(X, W, b)
        except Exception:
            answ = np.zeros(10)
        if equality_check(answ, expected, shape=expected.shape):
            print("‚úÖ Test Case 2: 6 features, ten neurons ... passed")
        else:
            print("‚ùå Test 2 Failed")
            msg = (
                "\nThe forward result is not the expected,"
                " when creating 10 neurons for six inputs the value was not the expected."
                "   - The order of matrix multiplication.\n"
                "   - Is the bias term present?\n"
                "   - The activation function is correctly applied.\n"
                "    - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n"
            )
            print(msg)

    # Test 3: Test that it work with any activation function.
    def test3():
        np.random.seed = 2021 # reproduce
        X = np.random.rand(6)
        W = np.random.rand( 10, 6)
        b = np.random.rand(10)

        g = lambda x: np.maximum(x, 0.0)

        expected = my_perceptron_forward(X, W, b, activation=g)
        try:
            answ = perceptron_forward(X, W, b, activation=g)
        except Exception:
            answ = np.zeros(10)
        if equality_check(answ, expected, shape=expected.shape):
            print("‚úÖ Test Case 3: custom activation function ... passed")
        else:
            print("‚ùå Test 3 Failed")
            msg = (
                "\nThe forward result is not the expected,"
                "when using an activation function diferent to the default value."
                "You might want to check the following:\n"
                "   - Are you using the parameter sent to the function 'activation' and not the default value 'sigmoid'.\n"
            )
            print(msg)
        
    # Test 4: Boundary cases single input, single output
    def test4():
        np.random.seed = 2000
        X = np.random.rand(1)
        W = np.random.rand(1,1)
        b = np.random.rand(1)

        expected = my_perceptron_forward(X, W, b, activation=sigmoid)
        try:
            answ = perceptron_forward(X, W, b, activation=sigmoid)
        except Exception as e:
            answ = np.zeros(10)
        if equality_check(answ,expected, shape=expected.shape):
            print("‚úÖ Test Case 4: Single input, single output ... passed.")
        else:
            print("‚ùå Test 4 Failed")
            msg = (
                "\nThe forward result is not the expected,"
                "when using a single input (X.shape=(1,)), and a single neuron (W.shape=(1,1))."
                "you might want to check the following:\n"
                "   - The order of matrix multiplication.\n"
                "   - Is the bias term present?\n"
                "   - The activation function is correctly applied.\n"
                "   - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n"
            )
            print(msg)

    # Test 5: Zero weights, zero bias
    def test5():
        np.random.seed = 2000
        X = np.random.rand(10)
        W = np.zeros((1,10)) 
        b = np.zeros(1)

        expected = my_perceptron_forward(X, W, b, activation=sigmoid)
        try:
            answ = perceptron_forward(X, W, b, activation=sigmoid)
        except Exception as e:
            answ = np.zeros(10)
        if equality_check(answ, expected, shape=expected.shape):
            print("‚úÖ Test Case 5: zero weights, zero bias ... passed")
        else:
            print("‚ùå Test 5 Failed")
            msg = (
                "\nThe forward result is not the expected,"
                "when using weights and bias zero. "
                "You might want to check the following:\n"
                "   - The order of matrix multiplication.\n"
                "   - Is the bias term present?\n"
                "   - The activation function is correctly applied.\n"
                "   - Did you use the np.dot or np.matmul to compute the matrix product correctly?\n"
            )
            print(msg)

    # Test -- regsiter that tests are passed.
    test1()
    test2()
    test3()
    test4()
    test5()
    # End test
"""

Improve it by adding useful comments if necessary.
