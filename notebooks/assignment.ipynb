{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa52b31",
   "metadata": {},
   "source": [
    "Sure! Here’s a version where all the LaTeX stays **inside** the collapsible `<details>`, so it’s cleaner and better contained when rendering:\n",
    "\n",
    "---\n",
    "\n",
    "# Task #1\n",
    "\n",
    "<details>\n",
    "<summary><strong>Overview of a Perceptron Layer</strong></summary>\n",
    "\n",
    "A simple linear feedforward, or perceptron, layer consists of nodes (neurons) that transform input data using weights, biases, and an activation function.\n",
    "\n",
    "### 1. Input Vector ($X$)\n",
    "\n",
    "The input layer consists of a vector:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where each $x_i$ is a feature of the input.  \n",
    "In this context, the vector will be a column vector; in other situations, row vectors are used.\n",
    "\n",
    "### 2. Weights ($W$)\n",
    "\n",
    "Each input is connected to the neurons in the perceptron layer through weights.  \n",
    "For a neuron $j$, the weights are represented as:\n",
    "\n",
    "$$\n",
    "W_j = [w_{1j}, w_{2j}, \\ldots, w_{nj}].\n",
    "$$\n",
    "\n",
    "In a complete layer (with $m$ neurons), all weights are stacked into a matrix:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "W_1 \\\\\n",
    "W_2 \\\\\n",
    "\\vdots \\\\\n",
    "W_m\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{n1} & w_{n2} & \\dots & w_{nm}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "### 3. Bias ($b$)\n",
    "\n",
    "Each neuron has an associated bias term, $b_j$, which allows the activation function to be shifted.  \n",
    "All bias terms are stored in a single bias vector $b$.\n",
    "\n",
    "### 4. Linear Transformation\n",
    "\n",
    "Each neuron computes a linear combination of its inputs:\n",
    "\n",
    "$$\n",
    "z_j = W_j \\cdot X + b_j.\n",
    "$$\n",
    "\n",
    "In matrix form (for multiple neurons):\n",
    "\n",
    "$$\n",
    "Z = W X + b,\n",
    "$$\n",
    "\n",
    "where $W$ is the weight matrix, $X$ is the input vector, and $b$ is the bias vector.\n",
    "\n",
    "### 5. Activation Function ($g$)\n",
    "\n",
    "The linear output $z_j$ is passed through a non-linear activation function (e.g., sigmoid, ReLU) to introduce non-linearity:\n",
    "\n",
    "$$\n",
    "a_j = g(z_j).\n",
    "$$\n",
    "\n",
    "For the sigmoid function:\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "### 6. Output\n",
    "\n",
    "The activation $a_j$ is either passed to the next layer or becomes the final output if it’s the last layer.  \n",
    "In vector form, the layer's output is:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = g\\left( W X + b \\right).\n",
    "$$\n",
    "\n",
    "You can stack several such layers to build a deeper neural network.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "# Instructions\n",
    "\n",
    "<details>\n",
    "<summary><strong>Implementation Guidelines</strong></summary>\n",
    "\n",
    "Your task is to implement forward propagation in a neural network, *as described above*.\n",
    "\n",
    "You'll compute the forward pass for a single perceptron layer using NumPy.\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "- The **input** will always be a vector of shape $(N,)$, where $N$ is the number of features.\n",
    "  - Although $X$ is treated as a column vector mathematically, it will be a 1D array in NumPy.\n",
    "  - Pay attention to how NumPy handles vector/matrix operations.\n",
    "\n",
    "- The **weight matrix** $W$ will have a shape $(m, N)$:\n",
    "  - Example: one neuron, two features → $W$ has shape $(1, 2)$.\n",
    "  - Example: two neurons, five features → $W$ has shape $(2, 5)$.\n",
    "\n",
    "- The **activation function** will default to 'sigmoid', but you should ensure it supports other activation functions as well.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:**  \n",
    "This explanation assumes feature vectors are column vectors.  \n",
    "In many deep learning frameworks (such as TensorFlow or PyTorch), feature vectors are treated as row vectors.  \n",
    "Always double-check the shape expectations when switching frameworks!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Forward pass for a single perceptron layer.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# The sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Write your code here\n",
    "def perceptron_forward(x, W, b, activation=sigmoid):\n",
    "    \"\"\"\n",
    "    Computes the passforward for a feture input X\n",
    "    \"\"\"\n",
    "    # Write your solution here!!!\n",
    "    # Change the following code #\n",
    "    # You can start by computing the linear transformation z = wX+B\n",
    "    z = # write your answer here!\n",
    "    # Then remember the activation function\n",
    "    a = # write your answer here!\n",
    "    \n",
    "    return a\n",
    "    # ######################### #\n",
    "\n",
    "# Before the test let's run the function to check how it works!\n",
    "\n",
    "# Weights and bias are given in each test\n",
    "# You will leran more about initialzing the weights later.\n",
    "W = np.array([[1, 0.5, 1]])\n",
    "# Notice this is a three input single output perceptron.\n",
    "b = np.array([-1.0]) \n",
    "# A sample feature vector\n",
    "X = np.array([0.9, 0.7, 0.3])\n",
    "\n",
    "print(f\"Shape of W: {W.shape}\")\n",
    "print(f\"Shape of bias vector: {b.shape}\")\n",
    "print(f\"Shape of input: {X.shape}\")\n",
    "\n",
    "\n",
    "a = perceptron_forward(X, W, b)\n",
    "print(f\"Activation output: {a}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9dc362",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "To test your implementation, simply execute the following cell.  \n",
    "The code will send the function you implemented to a library that will run several test cases.  \n",
    "If something goes wrong, it will provide feedback to help you identify and fix the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63553f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get your function tested.\n",
    "from assignment_test import test_task1\n",
    "\n",
    "test_task1(perceptron_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ee8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sections to come ...d\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
